---
title: "Statistical Hadronization Model Calculations of Heavy Flavor Hadron Production in Relativistic Heavy-Ion Collisions at RHIC and the LHC"
excerpt: "Summer 2023 Project <br/> <img src = '/images/3CFO_figure.png'>"
collection: portfolio
---

This work, supervised by Yale Professors Smita Krishnaswamy and Ian Adelstein, focused on understanding how different network architectures represent and process information, particularly in high-dimensional spaces. Hypothesizing that neural networks can be largely characterized by the data representations they create, we proposed to generate a low-dimensional manifold of neural networks organized by their hidden-layer representations of a dataset. My work involved training and fine-tuning ResNets and CNNs; characterizing the structure of their embeddings using diffusion spectral entropy (DSE), which measures the number of significant eigendirections of the data; and analyzing signals on the manifold. I found that high-performing neural networks—across different architectures—exhibit high DSE, which suggests a more pronounced cluster structure. Moreover, I showed that test accuracy was a low-frequency signal on the manifold, meaning that it is structured by network performance. This insight offers a more systematic method for architecture search: hyperparameters extrapolated from models in the high-accuracy region of the learned manifold could lead to high-performing models. We submitted a manuscript presenting our results at the 28th International Conference on Artificial Intelligence and Statistics in October 2024.

<a href='https://indico.cern.ch/event/1139644/contributions/5490409/'>Poster</a>

<a href='https://meetings.aps.org/Meeting/DNP24/Session/K03.8'>Abstract</a>
